# Hybrid-RAG-System-for-Domain-Specific-Knowledge-Retrieval
Production-grade Retrieval-Augmented Generation with Hybrid Search, Reranking, Query Rewriting & Multi-Hop Retrieval

## ğŸš€ Overview

This project implements a full RAG pipeline for domain-specific PDFs and documents, covering the complete workflow:
1. PDF ingestion & chunking â€“ extract text from uploaded PDFs and split into retrievable chunks.
2. Vector embeddings & storage â€“ generate embeddings using OpenAI embeddings, stored in Qdrant.
3. Hybrid retrieval â€“ combines BM25 sparse search + dense vector search.
4. Cross-encoder reranking â€“ selects the most relevant chunks.
5. Query rewriting â€“ automatically reformulates ambiguous questions.
6. Multi-hop retrieval â€“ expands context iteratively if more evidence is needed.
7. LLM-based answer generation â€“ produces grounded answers from retrieved chunks.
8. Graceful degradation â€“ fallback retrieval if advanced components fail.
9. Evaluation-ready architecture â€“ designed to measure retrieval and answer accuracy.

## ğŸ¯ Why This Project Matters
Most RAG implementations fail because:  
Vector similarity alone misses keyword-critical matches  
Retrieved chunks are poorly ranked  
Complex queries require reasoning across documents  
Systems collapse when embedding search fails  
  
This project solves those limitations using a layered retrieval architecture.  
  
## ğŸ§  System Architecture
```
            User Query
                â”‚
                â–¼
            Query Rewriter (LLM)
                â”‚
                â–¼
â”€â”€â”€â”€â”€â”€â”€â”€ Hybrid Retrieval â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                                  â”‚
â”‚   BM25 Sparse Search             â”‚
â”‚   (keyword precision)            â”‚
â”‚                                  â”‚
â”‚   Vector Search (Qdrant)         â”‚
â”‚   (semantic similarity)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
        Candidate Pool
               â”‚
               â–¼
        Cross-Encoder
           Reranker
               â”‚
               â–¼
        Multi-Hop Retrieval
      (context expansion)
               â”‚
               â–¼
        Final Context
               â”‚
               â–¼
            LLM Answer
```
## âœ¨ Key Features
âœ… PDF Ingestion
Upload PDFs via Streamlit or API.  
Automatic chunking into 1k-token overlapping segments.  
Assigns unique source_id for each document.  
   
âœ… Vector Embeddings & Storage  
Uses OpenAI text embeddings (text-embedding-3-small).  
Efficient storage in Qdrant for semantic search.  

âœ… Hybrid Retrieval  
Combines BM25 lexical search with vector similarity.  
Improves precision & recall across varied queries.  
  
âœ… Neural Reranking  
Cross-encoder selects the most relevant chunks from candidate pool.  
  
âœ… Automatic Query Rewriting  
Reformulates vague or underspecified questions for better retrieval.  
  
âœ… Multi-Hop Retrieval  
Iteratively fetches additional chunks when more evidence is needed.  
  
âœ… Graceful Degradation  
Secondary vector-only or BM25-only retrieval ensures uptime if primary pipeline fails.  
  
âœ… Modular, Production-Ready  
Each stage is independent for testing, scaling, or swapping models.  
  
## ğŸ§± Project Structure
```
RAG/
â”‚
â”œâ”€â”€ main.py                # Entry point
â”œâ”€â”€ retrieval_pipeline.py  # Full retrieval orchestration
â”œâ”€â”€ query_engine.py        # Query execution logic
â”œâ”€â”€ reranker.py            # Cross-encoder reranking
â”œâ”€â”€ bm25_index.py          # Sparse retrieval
â”œâ”€â”€ vector_db.py           # Qdrant interface
â”œâ”€â”€ data_loader.py         # Index builder
â”œâ”€â”€ streamlit_app.py       # UI demo
â”‚
â”œâ”€â”€ qdrant_storage/        # Local vector DB (ignored in git)
â””â”€â”€ README.md
```
## âš™ï¸ Retrieval Pipeline
Query rewritten for clarity  
Hybrid retrieval generates candidates  
Results merged and deduplicated  
Neural reranker scores relevance  
Multi-hop expansion retrieves missing context  
Final context passed to LLM  

## ğŸ“Š Evaluation (Planned)

Designed for benchmarking with:  
Recall@K  
MRR (Mean Reciprocal Rank)  
Answer Faithfulness  
Context Precision  
Evaluation module intentionally separated to allow dataset-agnostic testing.  

## ğŸ› ï¸ Tech Stack
Python 3.11  
FastAPI + Inngest for event-driven workflow  
Qdrant vector DB  
BM25 sparse search  
Cross-encoder reranker  
OpenAI embeddings & GPT-5 nano  
Streamlit UI for PDF upload and query  
Modular RAG architecture  

## â–¶ï¸ Quick Start
```bash
# Clone repo
git clone https://github.com/YOUR_USERNAME/RAG.git
cd RAG

# Install dependencies
pip install -r requirements.txt

# Start Qdrant
docker run -d --name qdrantRagDB -p 6333:6333 -v "$(pwd)/qdrant_storage:/qdrant/storage" qdrant/qdrant

# Run server (FastAPI + Inngest)
uvicorn main:app --reload

# Optional: Streamlit UI
streamlit run streamlit_app.py
```

## ğŸ“ˆ Engineering Highlights
Production-style retrieval orchestration  
Separation of retrieval vs reasoning layers  
Failure-resilient pipeline design  
Research-friendly experimentation structure  
Scalable to distributed vector databases  

## ğŸ§© Future Work
Retrieval evaluation dashboard  
Adaptive chunking  
Agentic retrieval planning  
Domain-specific fine-tuned reranker  

## ğŸ‘¤ Author
AI Engineer focused on information retrieval, document intelligence, and applied LLM systems.

## ğŸ“œ License
MIT License